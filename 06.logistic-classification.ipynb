{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dea1790-0bbd-4043-8fd0-ae9e1b3f966d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor(1.8380541, shape=(), dtype=float32)\n",
      "200 tf.Tensor(0.46197116, shape=(), dtype=float32)\n",
      "400 tf.Tensor(0.44029394, shape=(), dtype=float32)\n",
      "600 tf.Tensor(0.4225897, shape=(), dtype=float32)\n",
      "800 tf.Tensor(0.40708897, shape=(), dtype=float32)\n",
      "1000 tf.Tensor(0.3930001, shape=(), dtype=float32)\n",
      "1200 tf.Tensor(0.37992963, shape=(), dtype=float32)\n",
      "1400 tf.Tensor(0.36766735, shape=(), dtype=float32)\n",
      "1600 tf.Tensor(0.35609207, shape=(), dtype=float32)\n",
      "1800 tf.Tensor(0.3451271, shape=(), dtype=float32)\n",
      "2000 tf.Tensor(0.33471894, shape=(), dtype=float32)\n",
      "2200 tf.Tensor(0.3248271, shape=(), dtype=float32)\n",
      "2400 tf.Tensor(0.31541798, shape=(), dtype=float32)\n",
      "2600 tf.Tensor(0.30646226, shape=(), dtype=float32)\n",
      "2800 tf.Tensor(0.29793343, shape=(), dtype=float32)\n",
      "3000 tf.Tensor(0.2898072, shape=(), dtype=float32)\n",
      "3200 tf.Tensor(0.28206056, shape=(), dtype=float32)\n",
      "3400 tf.Tensor(0.27467203, shape=(), dtype=float32)\n",
      "3600 tf.Tensor(0.26762125, shape=(), dtype=float32)\n",
      "3800 tf.Tensor(0.26088917, shape=(), dtype=float32)\n",
      "4000 tf.Tensor(0.25445774, shape=(), dtype=float32)\n",
      "4200 tf.Tensor(0.2483099, shape=(), dtype=float32)\n",
      "4400 tf.Tensor(0.24242981, shape=(), dtype=float32)\n",
      "4600 tf.Tensor(0.23680234, shape=(), dtype=float32)\n",
      "4800 tf.Tensor(0.23141354, shape=(), dtype=float32)\n",
      "5000 tf.Tensor(0.22624993, shape=(), dtype=float32)\n",
      "5200 tf.Tensor(0.22129916, shape=(), dtype=float32)\n",
      "5400 tf.Tensor(0.21654959, shape=(), dtype=float32)\n",
      "5600 tf.Tensor(0.21199031, shape=(), dtype=float32)\n",
      "5800 tf.Tensor(0.20761102, shape=(), dtype=float32)\n",
      "6000 tf.Tensor(0.20340212, shape=(), dtype=float32)\n",
      "6200 tf.Tensor(0.19935463, shape=(), dtype=float32)\n",
      "6400 tf.Tensor(0.1954601, shape=(), dtype=float32)\n",
      "6600 tf.Tensor(0.19171049, shape=(), dtype=float32)\n",
      "6800 tf.Tensor(0.1880985, shape=(), dtype=float32)\n",
      "7000 tf.Tensor(0.18461709, shape=(), dtype=float32)\n",
      "7200 tf.Tensor(0.18125969, shape=(), dtype=float32)\n",
      "7400 tf.Tensor(0.17802012, shape=(), dtype=float32)\n",
      "7600 tf.Tensor(0.17489262, shape=(), dtype=float32)\n",
      "7800 tf.Tensor(0.17187177, shape=(), dtype=float32)\n",
      "8000 tf.Tensor(0.16895245, shape=(), dtype=float32)\n",
      "8200 tf.Tensor(0.16612977, shape=(), dtype=float32)\n",
      "8400 tf.Tensor(0.16339932, shape=(), dtype=float32)\n",
      "8600 tf.Tensor(0.16075675, shape=(), dtype=float32)\n",
      "8800 tf.Tensor(0.15819807, shape=(), dtype=float32)\n",
      "9000 tf.Tensor(0.15571947, shape=(), dtype=float32)\n",
      "9200 tf.Tensor(0.1533174, shape=(), dtype=float32)\n",
      "9400 tf.Tensor(0.15098837, shape=(), dtype=float32)\n",
      "9600 tf.Tensor(0.1487294, shape=(), dtype=float32)\n",
      "9800 tf.Tensor(0.14653717, shape=(), dtype=float32)\n",
      "10000 tf.Tensor(0.1444091, shape=(), dtype=float32)\n",
      "Hypothesis:  tf.Tensor(\n",
      "[[0.02863405]\n",
      " [0.1558898 ]\n",
      " [0.29462716]\n",
      " [0.78610003]\n",
      " [0.9425098 ]\n",
      " [0.9811584 ]], shape=(6, 1), dtype=float32) \n",
      "Correct (Y):  tf.Tensor(0.1444091, shape=(), dtype=float32) \n",
      "Accuracy:  tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = tf.constant([[1, 2],\n",
    "                      [2, 3],\n",
    "                      [3, 1],\n",
    "                      [4, 3],\n",
    "                      [5, 3],\n",
    "                      [6, 2]], dtype=tf.float32)\n",
    "y_data = tf.constant([[0],\n",
    "                      [0],\n",
    "                      [0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [1]], dtype=tf.float32)\n",
    "\n",
    "X = tf.Variable(0., shape=tf.TensorShape(None), dtype = tf.float32)\n",
    "Y = tf.Variable(0., shape=tf.TensorShape(None), dtype = tf.float32)\n",
    "W = tf.Variable(tf.random.normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# sigmoid function == tf.math.div(1. 1. + tf.math.exp(tf.linalg.matmul(X, W) + b))\n",
    "def hypothesis():\n",
    "    return tf.sigmoid(tf.linalg.matmul(X, W) + b)\n",
    "\n",
    "# 밥그릇 형태를 위한 2가지 로그함수 짬뽕 (Y값이 1과 0을 표현해야하므로)\n",
    "def cost(): \n",
    "    return -tf.reduce_mean(Y * tf.math.log(hypothesis()) + (1 - Y) * tf.math.log(1 - hypothesis()))\n",
    "\n",
    "# 경사하강법\n",
    "def train():\n",
    "    return tf.keras.optimizers.SGD(learning_rate = 0.01).minimize(cost, var_list=[W, b])\n",
    "\n",
    "# Hypothesis가 0.5 보다 크면 true, 외 false 처리하기 위해 float32로 캐스팅\n",
    "def predicted():\n",
    "    return tf.cast(hypothesis() > 0.5, dtype=tf.float32)\n",
    "\n",
    "# 정확도 계산, 예측값과 실 Y값이 얼마나 같은지를 평균\n",
    "def accuracy():\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predicted(), Y), dtype=tf.float32))\n",
    "\n",
    "X.assign(x_data)\n",
    "Y.assign(y_data)\n",
    "for step in range(10001):\n",
    "    train()\n",
    "    if step % 200 == 0:\n",
    "        print(step, cost())\n",
    "\n",
    "h, c, a = hypothesis(), cost(), accuracy()\n",
    "print(\"Hypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5084ea4a-acc9-4f8e-b54c-e780108b3c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3 (12.00 Byte)\n",
      "Trainable params: 3 (12.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "200/200 [==============================] - 0s 648us/step - loss: 0.5256 - accuracy: 0.8350\n",
      "Epoch 2/50\n",
      "200/200 [==============================] - 0s 617us/step - loss: 0.4973 - accuracy: 0.8350\n",
      "Epoch 3/50\n",
      "200/200 [==============================] - 0s 602us/step - loss: 0.4810 - accuracy: 0.8350\n",
      "Epoch 4/50\n",
      "200/200 [==============================] - 0s 611us/step - loss: 0.4684 - accuracy: 0.8300\n",
      "Epoch 5/50\n",
      "200/200 [==============================] - 0s 596us/step - loss: 0.4467 - accuracy: 0.8350\n",
      "Epoch 6/50\n",
      "200/200 [==============================] - 0s 596us/step - loss: 0.4355 - accuracy: 0.8300\n",
      "Epoch 7/50\n",
      "200/200 [==============================] - 0s 598us/step - loss: 0.4218 - accuracy: 0.8300\n",
      "Epoch 8/50\n",
      "200/200 [==============================] - 0s 606us/step - loss: 0.4071 - accuracy: 0.8300\n",
      "Epoch 9/50\n",
      "200/200 [==============================] - 0s 622us/step - loss: 0.3933 - accuracy: 0.8300\n",
      "Epoch 10/50\n",
      "200/200 [==============================] - 0s 613us/step - loss: 0.3743 - accuracy: 0.8350\n",
      "Epoch 11/50\n",
      "200/200 [==============================] - 0s 599us/step - loss: 0.3683 - accuracy: 0.8300\n",
      "Epoch 12/50\n",
      "200/200 [==============================] - 0s 587us/step - loss: 0.3519 - accuracy: 0.8350\n",
      "Epoch 13/50\n",
      "200/200 [==============================] - 0s 599us/step - loss: 0.3411 - accuracy: 0.8350\n",
      "Epoch 14/50\n",
      "200/200 [==============================] - 0s 607us/step - loss: 0.3309 - accuracy: 0.8350\n",
      "Epoch 15/50\n",
      "200/200 [==============================] - 0s 670us/step - loss: 0.3240 - accuracy: 0.8300\n",
      "Epoch 16/50\n",
      "200/200 [==============================] - 0s 604us/step - loss: 0.3106 - accuracy: 0.8350\n",
      "Epoch 17/50\n",
      "200/200 [==============================] - 0s 604us/step - loss: 0.3013 - accuracy: 0.8350\n",
      "Epoch 18/50\n",
      "200/200 [==============================] - 0s 607us/step - loss: 0.2931 - accuracy: 0.8350\n",
      "Epoch 19/50\n",
      "200/200 [==============================] - 0s 619us/step - loss: 0.2863 - accuracy: 0.8350\n",
      "Epoch 20/50\n",
      "200/200 [==============================] - 0s 616us/step - loss: 0.2792 - accuracy: 0.8350\n",
      "Epoch 21/50\n",
      "200/200 [==============================] - 0s 629us/step - loss: 0.2715 - accuracy: 0.8350\n",
      "Epoch 22/50\n",
      "200/200 [==============================] - 0s 677us/step - loss: 0.2644 - accuracy: 0.8350\n",
      "Epoch 23/50\n",
      "200/200 [==============================] - 0s 626us/step - loss: 0.2589 - accuracy: 0.8700\n",
      "Epoch 24/50\n",
      "200/200 [==============================] - 0s 629us/step - loss: 0.2510 - accuracy: 0.9250\n",
      "Epoch 25/50\n",
      "200/200 [==============================] - 0s 612us/step - loss: 0.2488 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "200/200 [==============================] - 0s 606us/step - loss: 0.2417 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "200/200 [==============================] - 0s 608us/step - loss: 0.2329 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "200/200 [==============================] - 0s 616us/step - loss: 0.2318 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "200/200 [==============================] - 0s 610us/step - loss: 0.2252 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "200/200 [==============================] - 0s 625us/step - loss: 0.2202 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "200/200 [==============================] - 0s 628us/step - loss: 0.2131 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "200/200 [==============================] - 0s 620us/step - loss: 0.2087 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "200/200 [==============================] - 0s 608us/step - loss: 0.2062 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "200/200 [==============================] - 0s 611us/step - loss: 0.2026 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "200/200 [==============================] - 0s 623us/step - loss: 0.1996 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "200/200 [==============================] - 0s 613us/step - loss: 0.1932 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "200/200 [==============================] - 0s 617us/step - loss: 0.1914 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "200/200 [==============================] - 0s 623us/step - loss: 0.1862 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "200/200 [==============================] - 0s 607us/step - loss: 0.1815 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "200/200 [==============================] - 0s 603us/step - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "200/200 [==============================] - 0s 621us/step - loss: 0.1752 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "200/200 [==============================] - 0s 628us/step - loss: 0.1739 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "200/200 [==============================] - 0s 608us/step - loss: 0.1692 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "200/200 [==============================] - 0s 630us/step - loss: 0.1673 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "200/200 [==============================] - 0s 615us/step - loss: 0.1643 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "200/200 [==============================] - 0s 623us/step - loss: 0.1607 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "200/200 [==============================] - 0s 610us/step - loss: 0.1593 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "200/200 [==============================] - 0s 596us/step - loss: 0.1574 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "200/200 [==============================] - 0s 597us/step - loss: 0.1541 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "200/200 [==============================] - 0s 606us/step - loss: 0.1536 - accuracy: 1.0000\n",
      "Accuracy:  1.0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predict:  [[0.03082051]\n",
      " [0.16144317]\n",
      " [0.30494657]\n",
      " [0.7862682 ]\n",
      " [0.9414548 ]\n",
      " [0.9806779 ]]\n"
     ]
    }
   ],
   "source": [
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2],\n",
    "          [2, 3],\n",
    "          [3, 1],\n",
    "          [4, 3],\n",
    "          [5, 3],\n",
    "          [6, 2]]\n",
    "y_data = [[0],\n",
    "          [0],\n",
    "          [0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=2))\n",
    "# use sigmoid activation for 0~1 problem\n",
    "tf.model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "''' \n",
    "better result with loss function == 'binary_crossentropy', try 'mse' for yourself\n",
    "adding accuracy metric to get accuracy report during training\n",
    "'''\n",
    "tf.model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_data, y_data, epochs=50, steps_per_epoch=200)\n",
    "\n",
    "# Accuracy report\n",
    "print(\"Accuracy: \", history.history['accuracy'][-1])\n",
    "\n",
    "# predict\n",
    "print(\"Predict: \", tf.model.predict(x_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0afa900-f796-4c81-bf77-d50105703a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Cost:  2.0653427\n",
      "0 200 Cost:  0.91490465\n",
      "0 400 Cost:  0.6127531\n",
      "0 600 Cost:  0.5562735\n",
      "0 800 Cost:  0.5404971\n",
      "0 1000 Cost:  0.53240657\n",
      "0 1200 Cost:  0.5263992\n",
      "0 1400 Cost:  0.5213564\n",
      "0 1600 Cost:  0.51698375\n",
      "0 1800 Cost:  0.51315624\n",
      "0 2000 Cost:  0.5097928\n",
      "1 0 Cost:  0.5932235\n",
      "1 200 Cost:  0.57819104\n",
      "1 400 Cost:  0.56632\n",
      "1 600 Cost:  0.55586517\n",
      "1 800 Cost:  0.5463861\n",
      "1 1000 Cost:  0.5377217\n",
      "1 1200 Cost:  0.52977437\n",
      "1 1400 Cost:  0.522467\n",
      "1 1600 Cost:  0.5157332\n",
      "1 1800 Cost:  0.50951445\n",
      "1 2000 Cost:  0.5037594\n",
      "2 0 Cost:  0.57251334\n",
      "2 200 Cost:  0.57106435\n",
      "2 400 Cost:  0.5700434\n",
      "2 600 Cost:  0.5691752\n",
      "2 800 Cost:  0.56839544\n",
      "2 1000 Cost:  0.56768465\n",
      "2 1200 Cost:  0.5670336\n",
      "2 1400 Cost:  0.56643575\n",
      "2 1600 Cost:  0.5658855\n",
      "2 1800 Cost:  0.5653779\n",
      "2 2000 Cost:  0.5649086\n",
      "3 0 Cost:  0.50466937\n",
      "3 200 Cost:  0.498627\n",
      "3 400 Cost:  0.4942278\n",
      "3 600 Cost:  0.49046153\n",
      "3 800 Cost:  0.48709682\n",
      "3 1000 Cost:  0.4840528\n",
      "3 1200 Cost:  0.4812825\n",
      "3 1400 Cost:  0.47874978\n",
      "3 1600 Cost:  0.47642508\n",
      "3 1800 Cost:  0.47428337\n",
      "3 2000 Cost:  0.47230324\n",
      "4 0 Cost:  0.49658325\n",
      "4 200 Cost:  0.4757678\n",
      "4 400 Cost:  0.46966457\n",
      "4 600 Cost:  0.46715695\n",
      "4 800 Cost:  0.46561393\n",
      "4 1000 Cost:  0.4643646\n",
      "4 1200 Cost:  0.46322918\n",
      "4 1400 Cost:  0.462157\n",
      "4 1600 Cost:  0.46113214\n",
      "4 1800 Cost:  0.46014825\n",
      "4 2000 Cost:  0.45920166\n",
      "5 0 Cost:  0.36286643\n",
      "5 200 Cost:  0.35883513\n",
      "5 400 Cost:  0.355243\n",
      "5 600 Cost:  0.35197553\n",
      "5 800 Cost:  0.34897816\n",
      "5 1000 Cost:  0.34621656\n",
      "5 1200 Cost:  0.34366444\n",
      "5 1400 Cost:  0.3413\n",
      "5 1600 Cost:  0.33910456\n",
      "5 1800 Cost:  0.33706164\n",
      "5 2000 Cost:  0.33515668\n",
      "6 0 Cost:  0.49251857\n",
      "6 200 Cost:  0.48787162\n",
      "6 400 Cost:  0.4849192\n",
      "6 600 Cost:  0.48254997\n",
      "6 800 Cost:  0.4804593\n",
      "6 1000 Cost:  0.47855607\n",
      "6 1200 Cost:  0.47680518\n",
      "6 1400 Cost:  0.47518718\n",
      "6 1600 Cost:  0.47368774\n",
      "6 1800 Cost:  0.47229496\n",
      "6 2000 Cost:  0.47099847\n",
      "7 0 Cost:  0.4781737\n",
      "7 200 Cost:  0.47010177\n",
      "7 400 Cost:  0.4660003\n",
      "7 600 Cost:  0.46300736\n",
      "7 800 Cost:  0.4604022\n",
      "7 1000 Cost:  0.45800322\n",
      "7 1200 Cost:  0.45575896\n",
      "7 1400 Cost:  0.45364916\n",
      "7 1600 Cost:  0.45166153\n",
      "7 1800 Cost:  0.44978634\n",
      "7 2000 Cost:  0.44801515\n",
      "Hypothesis:  tf.Tensor(\n",
      "[[0.851403  ]\n",
      " [0.4551556 ]\n",
      " [0.80048066]\n",
      " [0.25530782]\n",
      " [0.58512384]\n",
      " [0.8530102 ]\n",
      " [0.07996375]\n",
      " [0.25109512]\n",
      " [0.82395035]\n",
      " [0.82150763]\n",
      " [0.70394516]\n",
      " [0.92029965]\n",
      " [0.679431  ]\n",
      " [0.64813095]\n",
      " [0.69286513]\n",
      " [0.75355506]\n",
      " [0.5854517 ]\n",
      " [0.7897127 ]\n",
      " [0.46872512]\n",
      " [0.492587  ]\n",
      " [0.8937783 ]\n",
      " [0.82804835]\n",
      " [0.7365324 ]\n",
      " [0.18620317]\n",
      " [0.90220344]\n",
      " [0.83922935]\n",
      " [0.7946027 ]\n",
      " [0.7222894 ]\n",
      " [0.8933308 ]\n",
      " [0.797468  ]\n",
      " [0.67437017]\n",
      " [0.3505618 ]\n",
      " [0.89813524]\n",
      " [0.9256645 ]\n",
      " [0.31660584]\n",
      " [0.1424121 ]\n",
      " [0.8154057 ]\n",
      " [0.20555964]\n",
      " [0.78957397]\n",
      " [0.17426793]\n",
      " [0.36844608]\n",
      " [0.44856033]\n",
      " [0.76144123]\n",
      " [0.8749636 ]\n",
      " [0.06370769]\n",
      " [0.26632893]\n",
      " [0.616093  ]\n",
      " [0.4682393 ]\n",
      " [0.39574274]\n",
      " [0.75203264]\n",
      " [0.06575041]\n",
      " [0.95374113]\n",
      " [0.06573617]\n",
      " [0.9091566 ]\n",
      " [0.7244521 ]\n",
      " [0.61974454]\n",
      " [0.87554896]\n",
      " [0.62257   ]\n",
      " [0.9024183 ]], shape=(59, 1), dtype=float32) \n",
      "Correct (Y):  tf.Tensor(0.44801515, shape=(), dtype=float32) \n",
      "Accuracy:  tf.Tensor(0.779661, shape=(), dtype=float32)\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predict: [[0.6079865]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 당뇨병 예측 데이터 1~8->팩터, 9->당뇨X: 0 당뇨Y: 1\n",
    "dataset = tf.data.experimental.make_csv_dataset('resource/data-03-diabetes.csv', batch_size=100, num_epochs=1, shuffle=False, header=False, column_names=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "X = tf.Variable(1., dtype=tf.float32, shape=tf.TensorShape(None))\n",
    "Y = tf.Variable(1., dtype=tf.float32, shape=tf.TensorShape(None))\n",
    "\n",
    "W = tf.Variable(tf.random.normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# sigmoid function == tf.math.div(1. 1. + tf.math.exp(tf.linalg.matmul(X, W) + b))\n",
    "def hypothesis():\n",
    "    return tf.sigmoid(tf.linalg.matmul(X, W) + b)\n",
    "\n",
    "# 밥그릇 형태를 위한 2가지 로그함수 짬뽕 (Y값이 1과 0을 표현해야하므로)\n",
    "def cost(): \n",
    "    return -tf.reduce_mean(Y * tf.math.log(hypothesis()) + (1 - Y) * tf.math.log(1 - hypothesis()))\n",
    "\n",
    "# 경사하강법\n",
    "def train():\n",
    "    return tf.keras.optimizers.SGD(learning_rate = 0.01).minimize(cost, var_list=[W, b])\n",
    "\n",
    "# Hypothesis가 0.5 보다 크면 true, 외 false 처리하기 위해 float32로 캐스팅\n",
    "def predicted():\n",
    "    return tf.cast(hypothesis() > 0.5, dtype=tf.float32)\n",
    "\n",
    "# 정확도 계산, 예측값과 실 Y값이 얼마나 같은지를 평균\n",
    "def accuracy():\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(predicted(), Y), dtype=tf.float32))\n",
    "\n",
    "i = 0\n",
    "for dict in dataset.as_numpy_iterator():\n",
    "    temp = np.array(list(dict.values())).swapaxes(0, 1)\n",
    "    x_data = temp[:, :-1]\n",
    "    y_data = temp[:, [-1]]\n",
    "    X.assign(x_data)\n",
    "    Y.assign(y_data)\n",
    "    for step in range(2001):\n",
    "        train()\n",
    "        if step % 200 == 0:\n",
    "            print(i, step, \"Cost: \", cost().numpy())\n",
    "    i += 1\n",
    "\n",
    "h, c, a = hypothesis(), cost(), accuracy()\n",
    "print(\"Hypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n",
    "y_predict = tf.model.predict([[0.176471, 0.155779, 0, 0, 0, 0.052161, -0.952178, -0.733333]])\n",
    "print(\"Predict: {0}\".format(y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c1fd8c-321a-4765-a23c-f4ea9b954c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9 (36.00 Byte)\n",
      "Trainable params: 9 (36.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 0s 674us/step - loss: 0.6342 - accuracy: 0.6433\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 0s 637us/step - loss: 0.5962 - accuracy: 0.6909\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 0s 635us/step - loss: 0.5709 - accuracy: 0.7009\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 0s 613us/step - loss: 0.5772 - accuracy: 0.7046\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 0s 627us/step - loss: 0.5688 - accuracy: 0.6971\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 0s 630us/step - loss: 0.5469 - accuracy: 0.7272\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 0s 653us/step - loss: 0.5459 - accuracy: 0.7196\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 0s 661us/step - loss: 0.5481 - accuracy: 0.7309\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 0s 623us/step - loss: 0.5385 - accuracy: 0.7309\n",
      "Epoch 10/10\n",
      " 85/200 [===========>..................] - ETA: 0s - loss: 0.5453 - accuracy: 0.7000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2000 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 350us/step - loss: 0.5468 - accuracy: 0.7018\n",
      "Accuracy: 0.7017543911933899\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predict: [[0.6079865]]\n",
      "24/24 [==============================] - 0s 700us/step - loss: 0.5354 - accuracy: 0.7339\n",
      "loss: 0.5353662371635437, accuracy: 0.7338603138923645\n"
     ]
    }
   ],
   "source": [
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "xy = np.loadtxt('resource/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# multi-variable, x_data.shape[1] == feature counts == 8 in this case\n",
    "tf.model.add(tf.keras.layers.Dense(units=1, input_dim=x_data.shape[1], activation='sigmoid'))\n",
    "tf.model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=0.01),  metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_data, y_data, epochs=10, steps_per_epoch=200)\n",
    "\n",
    "# accuracy!\n",
    "print(\"Accuracy: {0}\".format(history.history['accuracy'][-1]))\n",
    "\n",
    "# predict a single data point\n",
    "y_predict = tf.model.predict([[0.176471, 0.155779, 0, 0, 0, 0.052161, -0.952178, -0.733333]])\n",
    "print(\"Predict: {0}\".format(y_predict))\n",
    "\n",
    "# evaluating model\n",
    "evaluate = tf.model.evaluate(x_data, y_data)\n",
    "print(\"loss: {0}, accuracy: {1}\".format(evaluate[0], evaluate[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27dec0ed-92e9-4f7c-9a63-ff6970671e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3774 - binary_accuracy: 0.9231\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3759 - binary_accuracy: 0.9231\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3744 - binary_accuracy: 0.9231\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3729 - binary_accuracy: 0.9231\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3713 - binary_accuracy: 0.9231\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3698 - binary_accuracy: 0.9231\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3683 - binary_accuracy: 0.9231\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3668 - binary_accuracy: 0.9231\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3653 - binary_accuracy: 0.9231\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3638 - binary_accuracy: 0.9231\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3623 - binary_accuracy: 0.9231\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3608 - binary_accuracy: 0.9231\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3593 - binary_accuracy: 0.9231\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3578 - binary_accuracy: 0.9231\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3563 - binary_accuracy: 0.9231\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3548 - binary_accuracy: 0.9231\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3533 - binary_accuracy: 0.9231\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3518 - binary_accuracy: 0.9231\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3503 - binary_accuracy: 0.9231\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3488 - binary_accuracy: 0.9231\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3474 - binary_accuracy: 0.9231\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3459 - binary_accuracy: 0.9231\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3444 - binary_accuracy: 0.9231\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3429 - binary_accuracy: 0.9231\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3415 - binary_accuracy: 0.9231\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3400 - binary_accuracy: 0.9231\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3385 - binary_accuracy: 0.9231\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3371 - binary_accuracy: 0.9231\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3356 - binary_accuracy: 0.9231\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3341 - binary_accuracy: 0.9231\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3327 - binary_accuracy: 0.9231\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3312 - binary_accuracy: 0.9231\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3298 - binary_accuracy: 0.9231\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3283 - binary_accuracy: 0.9231\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3269 - binary_accuracy: 0.9231\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3254 - binary_accuracy: 0.9231\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3240 - binary_accuracy: 0.9231\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3226 - binary_accuracy: 0.9231\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3211 - binary_accuracy: 0.9231\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3197 - binary_accuracy: 0.9231\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3183 - binary_accuracy: 0.9231\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3169 - binary_accuracy: 0.9231\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3155 - binary_accuracy: 0.9231\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3140 - binary_accuracy: 0.9231\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3126 - binary_accuracy: 0.9231\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3112 - binary_accuracy: 0.9231\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3098 - binary_accuracy: 0.9231\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3084 - binary_accuracy: 0.9231\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3070 - binary_accuracy: 0.9231\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3056 - binary_accuracy: 0.9231\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3042 - binary_accuracy: 0.9231\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3029 - binary_accuracy: 0.9231\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3015 - binary_accuracy: 0.9231\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3001 - binary_accuracy: 0.9231\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2987 - binary_accuracy: 0.9231\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2974 - binary_accuracy: 0.9231\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2960 - binary_accuracy: 0.9231\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2947 - binary_accuracy: 0.9231\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2933 - binary_accuracy: 0.9231\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2919 - binary_accuracy: 0.9231\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2906 - binary_accuracy: 0.9231\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2893 - binary_accuracy: 0.9231\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2879 - binary_accuracy: 0.9231\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2866 - binary_accuracy: 0.9231\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2853 - binary_accuracy: 0.9231\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2840 - binary_accuracy: 0.9231\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2826 - binary_accuracy: 0.9231\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2813 - binary_accuracy: 0.9231\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2800 - binary_accuracy: 0.9231\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2787 - binary_accuracy: 0.9231\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2774 - binary_accuracy: 0.9231\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2762 - binary_accuracy: 0.9231\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2749 - binary_accuracy: 0.9231\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2736 - binary_accuracy: 0.9231\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2723 - binary_accuracy: 0.9231\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2711 - binary_accuracy: 0.9231\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2698 - binary_accuracy: 0.9231\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2685 - binary_accuracy: 0.9231\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2673 - binary_accuracy: 0.9231\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2661 - binary_accuracy: 0.9231\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2648 - binary_accuracy: 0.9231\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2636 - binary_accuracy: 0.9231\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2624 - binary_accuracy: 0.9231\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2612 - binary_accuracy: 0.9231\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2599 - binary_accuracy: 0.9231\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2587 - binary_accuracy: 0.9231\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2575 - binary_accuracy: 0.9231\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2564 - binary_accuracy: 0.9231\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2552 - binary_accuracy: 0.9231\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2540 - binary_accuracy: 0.9231\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2528 - binary_accuracy: 0.9231\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2517 - binary_accuracy: 0.9231\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2505 - binary_accuracy: 0.9231\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2494 - binary_accuracy: 0.9231\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2482 - binary_accuracy: 0.9231\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2471 - binary_accuracy: 0.9231\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2460 - binary_accuracy: 0.9231\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2449 - binary_accuracy: 0.9231\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2438 - binary_accuracy: 0.9231\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2427 - binary_accuracy: 0.9231\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2416 - binary_accuracy: 0.9231\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2405 - binary_accuracy: 0.9231\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2394 - binary_accuracy: 0.9231\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2383 - binary_accuracy: 0.9231\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2373 - binary_accuracy: 0.9231\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2362 - binary_accuracy: 0.9231\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2352 - binary_accuracy: 0.9231\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2341 - binary_accuracy: 0.9231\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2331 - binary_accuracy: 0.9231\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2321 - binary_accuracy: 0.9231\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2311 - binary_accuracy: 0.9231\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2301 - binary_accuracy: 0.9231\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2291 - binary_accuracy: 0.9231\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2281 - binary_accuracy: 0.9231\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2272 - binary_accuracy: 0.9231\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2262 - binary_accuracy: 0.9231\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2252 - binary_accuracy: 0.9231\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2243 - binary_accuracy: 0.9231\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2234 - binary_accuracy: 0.9231\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2224 - binary_accuracy: 0.9231\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2215 - binary_accuracy: 0.9231\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2206 - binary_accuracy: 0.9231\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2197 - binary_accuracy: 0.9231\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2188 - binary_accuracy: 0.9231\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2179 - binary_accuracy: 0.9231\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2171 - binary_accuracy: 0.9231\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2162 - binary_accuracy: 0.9231\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2154 - binary_accuracy: 0.9231\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2145 - binary_accuracy: 0.9231\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2137 - binary_accuracy: 0.9231\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2129 - binary_accuracy: 0.9231\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2121 - binary_accuracy: 0.9231\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2113 - binary_accuracy: 0.9231\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2105 - binary_accuracy: 0.9231\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2097 - binary_accuracy: 0.9231\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2089 - binary_accuracy: 0.9231\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2082 - binary_accuracy: 0.9231\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2074 - binary_accuracy: 0.9231\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2067 - binary_accuracy: 0.9231\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2060 - binary_accuracy: 0.9231\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2052 - binary_accuracy: 0.9231\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2045 - binary_accuracy: 0.9231\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2038 - binary_accuracy: 0.9231\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2031 - binary_accuracy: 0.9231\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2025 - binary_accuracy: 0.9231\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2018 - binary_accuracy: 0.9231\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2011 - binary_accuracy: 0.9231\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2005 - binary_accuracy: 0.9231\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1999 - binary_accuracy: 0.9231\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1992 - binary_accuracy: 0.9231\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1986 - binary_accuracy: 0.9231\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1980 - binary_accuracy: 0.9231\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1974 - binary_accuracy: 0.9231\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1968 - binary_accuracy: 0.9231\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1962 - binary_accuracy: 0.9231\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1957 - binary_accuracy: 0.9231\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1951 - binary_accuracy: 0.9231\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1946 - binary_accuracy: 0.9231\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1940 - binary_accuracy: 0.9231\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1935 - binary_accuracy: 0.9231\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1930 - binary_accuracy: 0.9231\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1925 - binary_accuracy: 0.9231\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1920 - binary_accuracy: 0.9231\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1915 - binary_accuracy: 0.9231\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1910 - binary_accuracy: 0.9231\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1905 - binary_accuracy: 0.9231\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1901 - binary_accuracy: 0.9231\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1896 - binary_accuracy: 0.9231\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1892 - binary_accuracy: 0.9231\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1887 - binary_accuracy: 0.9231\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1883 - binary_accuracy: 0.9231\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1879 - binary_accuracy: 0.9231\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1875 - binary_accuracy: 0.9231\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1871 - binary_accuracy: 0.9231\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1867 - binary_accuracy: 0.9231\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1863 - binary_accuracy: 0.9231\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1859 - binary_accuracy: 0.9231\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1856 - binary_accuracy: 0.9231\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1852 - binary_accuracy: 0.9231\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1848 - binary_accuracy: 0.9231\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1845 - binary_accuracy: 0.9231\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1842 - binary_accuracy: 0.9231\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1838 - binary_accuracy: 0.9231\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1835 - binary_accuracy: 0.9231\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1832 - binary_accuracy: 0.9231\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1829 - binary_accuracy: 0.9231\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1826 - binary_accuracy: 0.9231\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1823 - binary_accuracy: 0.9231\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1820 - binary_accuracy: 0.9231\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1817 - binary_accuracy: 0.9231\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1814 - binary_accuracy: 0.9231\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1811 - binary_accuracy: 0.9231\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1809 - binary_accuracy: 0.9231\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1806 - binary_accuracy: 0.9231\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1804 - binary_accuracy: 0.9231\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1801 - binary_accuracy: 0.9231\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1799 - binary_accuracy: 0.9231\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1796 - binary_accuracy: 0.9231\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1794 - binary_accuracy: 0.9231\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1792 - binary_accuracy: 0.9231\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1789 - binary_accuracy: 0.9231\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1787 - binary_accuracy: 0.9231\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1785 - binary_accuracy: 0.9231\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1783 - binary_accuracy: 0.9231\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1781 - binary_accuracy: 0.9231\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1779 - binary_accuracy: 0.9231\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1777 - binary_accuracy: 0.9231\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1775 - binary_accuracy: 0.9231\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1773 - binary_accuracy: 0.9231\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1771 - binary_accuracy: 0.9231\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1769 - binary_accuracy: 0.9231\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1768 - binary_accuracy: 0.9231\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1766 - binary_accuracy: 0.9231\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1764 - binary_accuracy: 0.9231\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1762 - binary_accuracy: 0.9231\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1761 - binary_accuracy: 0.9231\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1759 - binary_accuracy: 0.9231\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1757 - binary_accuracy: 0.9231\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1756 - binary_accuracy: 0.9231\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1754 - binary_accuracy: 0.9231\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1753 - binary_accuracy: 0.9231\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1751 - binary_accuracy: 0.9231\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1750 - binary_accuracy: 0.9231\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1748 - binary_accuracy: 0.9231\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1747 - binary_accuracy: 0.9231\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1746 - binary_accuracy: 0.9231\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1744 - binary_accuracy: 0.9231\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1743 - binary_accuracy: 0.9231\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1741 - binary_accuracy: 0.9231\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1740 - binary_accuracy: 0.9231\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1739 - binary_accuracy: 0.9231\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1737 - binary_accuracy: 0.9231\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1736 - binary_accuracy: 0.9231\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1735 - binary_accuracy: 0.9231\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1734 - binary_accuracy: 0.9231\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1732 - binary_accuracy: 0.9231\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1731 - binary_accuracy: 0.9231\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1730 - binary_accuracy: 0.9231\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1729 - binary_accuracy: 0.9231\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1728 - binary_accuracy: 0.9231\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1726 - binary_accuracy: 0.9231\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1725 - binary_accuracy: 0.9231\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1724 - binary_accuracy: 0.9231\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1723 - binary_accuracy: 0.9231\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1722 - binary_accuracy: 0.9231\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1721 - binary_accuracy: 0.9231\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1720 - binary_accuracy: 0.9231\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1718 - binary_accuracy: 0.9231\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1717 - binary_accuracy: 0.9231\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1716 - binary_accuracy: 0.9231\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1715 - binary_accuracy: 0.9231\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1714 - binary_accuracy: 0.9231\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1713 - binary_accuracy: 0.9231\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1712 - binary_accuracy: 0.9231\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1711 - binary_accuracy: 0.9231\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1710 - binary_accuracy: 0.9231\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1709 - binary_accuracy: 0.9231\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1708 - binary_accuracy: 0.9231\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1707 - binary_accuracy: 0.9231\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1706 - binary_accuracy: 0.9231\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1705 - binary_accuracy: 0.9231\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1704 - binary_accuracy: 0.9231\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1703 - binary_accuracy: 0.9231\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1702 - binary_accuracy: 0.9231\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1701 - binary_accuracy: 0.9231\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1700 - binary_accuracy: 0.9231\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1699 - binary_accuracy: 0.9231\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1698 - binary_accuracy: 0.9231\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1697 - binary_accuracy: 0.9231\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1696 - binary_accuracy: 0.9231\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1695 - binary_accuracy: 0.9231\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1694 - binary_accuracy: 0.9231\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1693 - binary_accuracy: 0.9231\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1692 - binary_accuracy: 0.9231\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1691 - binary_accuracy: 0.9231\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1690 - binary_accuracy: 0.9231\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1689 - binary_accuracy: 0.9231\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1688 - binary_accuracy: 0.9231\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1687 - binary_accuracy: 0.9231\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1686 - binary_accuracy: 0.9231\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1685 - binary_accuracy: 0.9231\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1684 - binary_accuracy: 0.9231\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1683 - binary_accuracy: 0.9231\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1682 - binary_accuracy: 0.9231\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1681 - binary_accuracy: 0.9231\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1680 - binary_accuracy: 0.9231\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1679 - binary_accuracy: 0.9231\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1678 - binary_accuracy: 0.9231\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1677 - binary_accuracy: 0.9231\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1677 - binary_accuracy: 0.9231\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1676 - binary_accuracy: 0.9231\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1675 - binary_accuracy: 0.9231\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1674 - binary_accuracy: 0.9231\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1673 - binary_accuracy: 0.9231\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1672 - binary_accuracy: 0.9231\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1671 - binary_accuracy: 0.9231\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1670 - binary_accuracy: 0.9231\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1669 - binary_accuracy: 0.9231\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1668 - binary_accuracy: 0.9231\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1667 - binary_accuracy: 0.9231\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1666 - binary_accuracy: 0.9231\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1665 - binary_accuracy: 0.9231\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1665 - binary_accuracy: 0.9231\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1664 - binary_accuracy: 0.9231\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1663 - binary_accuracy: 0.9231\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1662 - binary_accuracy: 0.9231\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1661 - binary_accuracy: 0.9231\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1660 - binary_accuracy: 0.9231\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1659 - binary_accuracy: 0.9231\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1658 - binary_accuracy: 0.9231\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1657 - binary_accuracy: 0.9231\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1656 - binary_accuracy: 0.9231\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1656 - binary_accuracy: 0.9231\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1655 - binary_accuracy: 0.9231\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1654 - binary_accuracy: 0.9231\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1653 - binary_accuracy: 0.9231\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1652 - binary_accuracy: 0.9231\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1651 - binary_accuracy: 0.9231\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1650 - binary_accuracy: 0.9231\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1649 - binary_accuracy: 0.9231\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1648 - binary_accuracy: 0.9231\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1647 - binary_accuracy: 0.9231\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1647 - binary_accuracy: 0.9231\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1646 - binary_accuracy: 0.9231\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1645 - binary_accuracy: 0.9231\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1644 - binary_accuracy: 0.9231\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1643 - binary_accuracy: 0.9231\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1642 - binary_accuracy: 0.9231\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1641 - binary_accuracy: 0.9231\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1640 - binary_accuracy: 0.9231\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1640 - binary_accuracy: 0.9231\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1639 - binary_accuracy: 0.9231\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1638 - binary_accuracy: 0.9231\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1637 - binary_accuracy: 0.9231\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1636 - binary_accuracy: 0.9231\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1635 - binary_accuracy: 0.9231\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1634 - binary_accuracy: 0.9231\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1633 - binary_accuracy: 0.9231\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1633 - binary_accuracy: 0.9231\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1632 - binary_accuracy: 0.9231\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1631 - binary_accuracy: 0.9231\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1630 - binary_accuracy: 0.9231\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1629 - binary_accuracy: 0.9231\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1628 - binary_accuracy: 0.9231\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1627 - binary_accuracy: 0.9231\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1627 - binary_accuracy: 0.9231\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1626 - binary_accuracy: 0.9231\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1625 - binary_accuracy: 0.9231\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1624 - binary_accuracy: 0.9231\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1623 - binary_accuracy: 0.9231\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1622 - binary_accuracy: 0.9231\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1621 - binary_accuracy: 0.9231\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1621 - binary_accuracy: 0.9231\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1620 - binary_accuracy: 0.9231\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1619 - binary_accuracy: 0.9231\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1618 - binary_accuracy: 0.9231\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1617 - binary_accuracy: 0.9231\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1616 - binary_accuracy: 0.9231\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1615 - binary_accuracy: 0.9231\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1615 - binary_accuracy: 0.9231\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1614 - binary_accuracy: 0.9231\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1613 - binary_accuracy: 0.9231\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1612 - binary_accuracy: 0.9231\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1611 - binary_accuracy: 0.9231\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1610 - binary_accuracy: 0.9231\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1610 - binary_accuracy: 0.9231\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1609 - binary_accuracy: 0.9231\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1608 - binary_accuracy: 0.9231\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1607 - binary_accuracy: 0.9231\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1606 - binary_accuracy: 0.9231\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1605 - binary_accuracy: 0.9231\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1605 - binary_accuracy: 0.9231\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1604 - binary_accuracy: 0.9231\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1603 - binary_accuracy: 0.9231\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1602 - binary_accuracy: 0.9231\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1601 - binary_accuracy: 0.9231\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1600 - binary_accuracy: 0.9231\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1600 - binary_accuracy: 0.9231\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1599 - binary_accuracy: 0.9231\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1598 - binary_accuracy: 0.9231\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1597 - binary_accuracy: 0.9231\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1596 - binary_accuracy: 0.9231\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1595 - binary_accuracy: 0.9231\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1595 - binary_accuracy: 0.9231\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1594 - binary_accuracy: 0.9231\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1593 - binary_accuracy: 0.9231\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1592 - binary_accuracy: 0.9231\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1591 - binary_accuracy: 0.9231\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1591 - binary_accuracy: 0.9231\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1590 - binary_accuracy: 0.9231\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1589 - binary_accuracy: 0.9231\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1588 - binary_accuracy: 0.9231\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1587 - binary_accuracy: 0.9231\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1586 - binary_accuracy: 0.9231\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1586 - binary_accuracy: 0.9231\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1585 - binary_accuracy: 0.9231\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1584 - binary_accuracy: 0.9231\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1583 - binary_accuracy: 0.9231\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1582 - binary_accuracy: 0.9231\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1582 - binary_accuracy: 0.9231\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1581 - binary_accuracy: 0.9231\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1580 - binary_accuracy: 0.9231\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1579 - binary_accuracy: 0.9231\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1578 - binary_accuracy: 0.9231\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1578 - binary_accuracy: 0.9231\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1577 - binary_accuracy: 0.9231\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1576 - binary_accuracy: 0.9231\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1575 - binary_accuracy: 0.9231\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1574 - binary_accuracy: 0.9231\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1574 - binary_accuracy: 0.9231\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1573 - binary_accuracy: 0.9231\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1572 - binary_accuracy: 0.9231\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1571 - binary_accuracy: 0.9231\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1570 - binary_accuracy: 0.9231\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1570 - binary_accuracy: 0.9231\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1569 - binary_accuracy: 0.9231\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1568 - binary_accuracy: 0.9231\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1567 - binary_accuracy: 0.9231\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1566 - binary_accuracy: 0.9231\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1566 - binary_accuracy: 0.9231\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1565 - binary_accuracy: 0.9231\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1564 - binary_accuracy: 0.9231\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1563 - binary_accuracy: 0.9231\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1563 - binary_accuracy: 0.9231\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1562 - binary_accuracy: 0.9231\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1561 - binary_accuracy: 0.9231\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1560 - binary_accuracy: 0.9231\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1559 - binary_accuracy: 0.9231\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1559 - binary_accuracy: 0.9231\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1558 - binary_accuracy: 0.9231\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1557 - binary_accuracy: 0.9231\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1556 - binary_accuracy: 0.9231\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1556 - binary_accuracy: 0.9231\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1555 - binary_accuracy: 0.9231\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1554 - binary_accuracy: 0.9231\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1553 - binary_accuracy: 0.9231\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1552 - binary_accuracy: 0.9231\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1552 - binary_accuracy: 0.9231\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1551 - binary_accuracy: 0.9231\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1550 - binary_accuracy: 0.9231\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1549 - binary_accuracy: 0.9231\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1549 - binary_accuracy: 0.9231\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1548 - binary_accuracy: 0.9231\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1547 - binary_accuracy: 0.9231\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1546 - binary_accuracy: 0.9231\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1546 - binary_accuracy: 0.9231\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1545 - binary_accuracy: 0.9231\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1544 - binary_accuracy: 0.9231\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1543 - binary_accuracy: 0.9231\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1542 - binary_accuracy: 0.9231\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1542 - binary_accuracy: 0.9231\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1541 - binary_accuracy: 0.9231\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1540 - binary_accuracy: 0.9231\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1539 - binary_accuracy: 0.9231\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1539 - binary_accuracy: 0.9231\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1538 - binary_accuracy: 0.9231\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1537 - binary_accuracy: 0.9231\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1536 - binary_accuracy: 0.9231\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1536 - binary_accuracy: 0.9231\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1535 - binary_accuracy: 0.9231\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1534 - binary_accuracy: 0.9231\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1533 - binary_accuracy: 0.9231\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1533 - binary_accuracy: 0.9231\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1532 - binary_accuracy: 0.9231\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1531 - binary_accuracy: 0.9231\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1530 - binary_accuracy: 0.9231\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1530 - binary_accuracy: 0.9231\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1529 - binary_accuracy: 0.9231\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1528 - binary_accuracy: 0.9231\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1527 - binary_accuracy: 0.9231\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1527 - binary_accuracy: 0.9231\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1526 - binary_accuracy: 0.9231\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1525 - binary_accuracy: 0.9231\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1524 - binary_accuracy: 0.9231\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1524 - binary_accuracy: 0.9231\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1523 - binary_accuracy: 0.9231\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1522 - binary_accuracy: 0.9231\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1522 - binary_accuracy: 0.9231\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1521 - binary_accuracy: 0.9231\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1520 - binary_accuracy: 0.9231\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1519 - binary_accuracy: 0.9231\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1519 - binary_accuracy: 0.9231\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1518 - binary_accuracy: 0.9231\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1517 - binary_accuracy: 0.9231\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1516 - binary_accuracy: 0.9231\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1516 - binary_accuracy: 0.9231\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1515 - binary_accuracy: 0.9231\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1514 - binary_accuracy: 0.9231\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1513 - binary_accuracy: 0.9231\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1513 - binary_accuracy: 0.9231\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1512 - binary_accuracy: 0.9231\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1511 - binary_accuracy: 0.9231\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1511 - binary_accuracy: 0.9231\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1510 - binary_accuracy: 0.9231\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1509 - binary_accuracy: 0.9231\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1508 - binary_accuracy: 0.9231\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1508 - binary_accuracy: 0.9231\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1507 - binary_accuracy: 0.9231\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1506 - binary_accuracy: 0.9231\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1506 - binary_accuracy: 0.9231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f833c510350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "x = np.array([-50, -40, -30, -20, -10, -5, 0, 5, 10, 20, 30, 40, 50])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1, activation='sigmoid'))\n",
    "\n",
    "sgd = optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(x, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580c2dc0-322e-4b22-9fe7-c89367bfe38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f833c444720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f833c4bce10>,\n",
       " <matplotlib.lines.Line2D at 0x7f836034cf10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, model.predict(x), 'b', x, y, 'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bdc282a-24a0-4900-9b0f-b08ba719307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f833c444720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.43278423]\n",
      " [0.4898806 ]\n",
      " [0.5472422 ]\n",
      " [0.603376  ]\n",
      " [0.6305461 ]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[0.88387764]\n",
      " [0.9870016 ]\n",
      " [0.9986816 ]\n",
      " [0.9998677 ]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([1, 2, 3, 4, 4.5]))\n",
    "print(model.predict([11, 21, 31, 41, 500]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
